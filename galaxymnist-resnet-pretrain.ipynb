{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:13:40.005484Z",
     "iopub.status.busy": "2024-03-14T11:13:40.004768Z",
     "iopub.status.idle": "2024-03-14T11:13:49.740420Z",
     "shell.execute_reply": "2024-03-14T11:13:49.739510Z",
     "shell.execute_reply.started": "2024-03-14T11:13:40.005451Z"
    },
    "id": "yZ_qgoHq-CtO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "import cv2\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "\n",
    "%pip install git+https://github.com/mwalmsley/galaxy_mnist.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:13:49.743071Z",
     "iopub.status.busy": "2024-03-14T11:13:49.742110Z",
     "iopub.status.idle": "2024-03-14T11:13:49.777742Z",
     "shell.execute_reply": "2024-03-14T11:13:49.776714Z",
     "shell.execute_reply.started": "2024-03-14T11:13:49.743041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:08:07.857496Z",
     "iopub.status.busy": "2024-03-14T12:08:07.857113Z",
     "iopub.status.idle": "2024-03-14T12:08:22.444066Z",
     "shell.execute_reply": "2024-03-14T12:08:22.442694Z",
     "shell.execute_reply.started": "2024-03-14T12:08:07.857467Z"
    }
   },
   "outputs": [],
   "source": [
    "HighRes = False \n",
    "\n",
    "if HighRes == False:\n",
    "    from galaxy_mnist import GalaxyMNIST\n",
    "    \n",
    "    #  Import galaxyMNIST dataset\n",
    "    tv_dataset = GalaxyMNIST(\n",
    "        root='ML_DP/gal_mnist', #change root with desired folder where to download the catalogue\n",
    "        download=True,\n",
    "        train=True  # by default, or False for canonical test set\n",
    "        )\n",
    "\n",
    "    test_dataset = GalaxyMNIST(\n",
    "        root='ML_DP/gal_mnist',  #change root with desired folder where to download the catalogue\n",
    "        download=True,\n",
    "        train=False  # by default, or False for canonical test set\n",
    "        )\n",
    "if HighRes == True:    \n",
    "    from galaxy_mnist import GalaxyMNISTHighrez\n",
    "    \n",
    "    #  Import galaxyMNISTHighrez dataset\n",
    "    tv_dataset = GalaxyMNISTHighrez(\n",
    "        root='ML_DP/gal_mnist', #change root with desired folder where to download the catalogue\n",
    "        download=True,\n",
    "        train=True  # by default, or False for canonical test set\n",
    "    )\n",
    "\n",
    "    test_dataset = GalaxyMNISTHighrez(\n",
    "        root='ML_DP/gal_mnist', #change root with desired folder where to download the catalogue\n",
    "        download=True,\n",
    "        train=False  # by default, or False for canonical test set\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:08:44.181175Z",
     "iopub.status.busy": "2024-03-14T12:08:44.180515Z",
     "iopub.status.idle": "2024-03-14T12:09:53.188334Z",
     "shell.execute_reply": "2024-03-14T12:09:53.187367Z",
     "shell.execute_reply.started": "2024-03-14T12:08:44.181142Z"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset in train, validation and test sets\n",
    "\n",
    "tv_images = tv_dataset.data\n",
    "tv_labels = tv_dataset.targets\n",
    "\n",
    "test_images = test_dataset.data\n",
    "test_labels = test_dataset.targets\n",
    "\n",
    "\n",
    "# Split in validation and train datasets\n",
    "# images_tv, images_test, y_tv, y_test  = train_test_split(images, labels, shuffle=True, test_size=0.2, random_state=123)\n",
    "images_train, images_val, y_train, y_val  = train_test_split(tv_images, tv_labels, shuffle=True, test_size=0.25, random_state=123)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "train_dataset = TensorDataset(images_train, y_train)\n",
    "val_dataset = TensorDataset(images_val, y_val)\n",
    "test_dataset = TensorDataset(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:09:53.190723Z",
     "iopub.status.busy": "2024-03-14T12:09:53.190291Z",
     "iopub.status.idle": "2024-03-14T12:09:54.607017Z",
     "shell.execute_reply": "2024-03-14T12:09:54.605965Z",
     "shell.execute_reply.started": "2024-03-14T12:09:53.190666Z"
    },
    "id": "0CcQCMWT-Ctm",
    "outputId": "4b966819-c9b2-4e39-9249-5ff53837d74b"
   },
   "outputs": [],
   "source": [
    "#selecting the ResNet model\n",
    "\n",
    "chosen_model = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in chosen_model.parameters(): #freezing the backbone\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# define out fully connected and classifier\n",
    "fully_connected = nn.Sequential(\n",
    "   nn.Linear(in_features=512, out_features=512), #if using ResNet50 or more complex models, change 'in_features' variable to 2048\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(in_features=512, out_features=256),\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(in_features=256, out_features=4) #number of classes = 4\n",
    ")\n",
    "\n",
    "# replace model class classifier attribute:\n",
    "chosen_model.fc = fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:09:57.963617Z",
     "iopub.status.busy": "2024-03-14T12:09:57.963189Z",
     "iopub.status.idle": "2024-03-14T12:09:57.996832Z",
     "shell.execute_reply": "2024-03-14T12:09:57.995626Z",
     "shell.execute_reply.started": "2024-03-14T12:09:57.963590Z"
    },
    "id": "0CcQCMWT-Ctm",
    "outputId": "4b966819-c9b2-4e39-9249-5ff53837d74b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define training function\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, test_dataset, device,\n",
    "                lr=0.0001, epochs=30, batch_size=32, l2=0.00001, patience=5, gamma=0.5,\n",
    "                 freezed_epochs = 30):\n",
    "    '''Parameters:\n",
    "    model= chosen model to train,\n",
    "    train_dataset, val_dataset, test_dataset= train, validation and test dataset,\n",
    "    device= device performing the training,\n",
    "    lr= initial learning rate size,\n",
    "    epochs= total number of epochs of training,\n",
    "    batch_size= gives the size of the batch used in each in each epoch (dataset_size/batch_size = number of objects used in an epoch),\n",
    "    l2= weight decay of learning rate,\n",
    "    patience= number of epochs between each rescaling of learning rate\n",
    "    gamma= rescaling factor of learning rate,\n",
    "    freezed_epochs= number of epochs without training of convolutional layers''' \n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # construct dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # creating a python dictionary with all the training history\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    # set up loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=l2, momentum = 0.9 )  # pass in the parameters to be updated and learning rate\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=patience, gamma=gamma)\n",
    "    \n",
    "    #saving the learning rate values during training\n",
    "    lear_rate = []\n",
    "    \n",
    "    \n",
    "\n",
    "    # Training Loop\n",
    "    print(\"Training Start:\")\n",
    "    for epoch in range(freezed_epochs):\n",
    "        model.train()  # start to train the model, activate training behavior\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # reshape images\n",
    "            images = images.to(device)\n",
    "            images = images.float() \n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(images)  # forward\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "            cur_train_loss = criterion(outputs, labels)  # loss\n",
    "            cur_train_acc = (pred == labels).sum().item() / batch_size\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()       # set gradient to zero, avoid gradient accumulating\n",
    "            \n",
    "            cur_train_loss.backward()   # run back propagation\n",
    "            optimizer.step()            # optimizer update all model parameters\n",
    "\n",
    "            # loss\n",
    "            train_loss += cur_train_loss\n",
    "            train_acc += cur_train_acc\n",
    "\n",
    "        # valid\n",
    "        model.eval()  # start to train the model, activate training behavior\n",
    "        with torch.no_grad():  # parameters not updated\n",
    "            for images, labels in val_loader:\n",
    "                \n",
    "                # calculate validation loss\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                images = images.float()\n",
    "        \n",
    "                outputs = model(images)\n",
    "\n",
    "                # loss\n",
    "                cur_valid_loss = criterion(outputs, labels)\n",
    "                val_loss += cur_valid_loss\n",
    "                \n",
    "                # acc\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                val_acc += (pred == labels).sum().item() / batch_size\n",
    "\n",
    "        # learning schedule step\n",
    "        scheduler.step()\n",
    "\n",
    "        # print training feedback\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_acc / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_acc / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch:{epoch + 1} / {epochs}, lr: {optimizer.param_groups[0]['lr']:.5f} train loss:{train_loss:.5f}, train acc: {train_acc:.5f}, valid loss:{val_loss:.5f}, valid acc:{val_acc:.5f}\")\n",
    "\n",
    "        # update all values and learning rate\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        lear_rate.append(scheduler.get_last_lr())\n",
    "        \n",
    "    #defreezing backbone    \n",
    "    for param in model.parameters(): \n",
    "        param.requires_grad = True\n",
    "\n",
    "    #Same training loop as before, this time the parameters of convolutional layers are trained too    \n",
    "    print(\"De-freezed:\")\n",
    "    for epoch in range(epochs-freezed_epochs):\n",
    "        model.train()  \n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # reshape images\n",
    "            images = images.to(device)\n",
    "            images = images.float()\n",
    "            labels = labels.to(device)  \n",
    "            \n",
    "            outputs = model(images)  \n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "            cur_train_loss = criterion(outputs, labels)  # loss\n",
    "            cur_train_acc = (pred == labels).sum().item() / batch_size\n",
    "\n",
    "            # backward\n",
    "            cur_train_loss.backward()   \n",
    "            optimizer.step()            \n",
    "            optimizer.zero_grad()       \n",
    "\n",
    "            # loss\n",
    "            train_loss += cur_train_loss\n",
    "            train_acc += cur_train_acc\n",
    "\n",
    "        # valid\n",
    "        model.eval()  \n",
    "        with torch.no_grad(): \n",
    "            for images, labels in val_loader:\n",
    "                \n",
    "                # calculate validation loss\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                images = images.float()\n",
    "                \n",
    "                outputs = model(images)\n",
    "\n",
    "                # loss\n",
    "                cur_valid_loss = criterion(outputs, labels)\n",
    "                val_loss += cur_valid_loss\n",
    "                \n",
    "                # acc\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                val_acc += (pred == labels).sum().item() / batch_size\n",
    "\n",
    "        # learning schedule step\n",
    "        scheduler.step()\n",
    "\n",
    "        # print training feedback\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_acc / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_acc / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch:{epoch + freezed_epochs + 1} / {epochs}, lr: {optimizer.param_groups[0]['lr']:.5f} train loss:{train_loss:.5f}, train acc: {train_acc:.5f}, valid loss:{val_loss:.5f}, valid acc:{val_acc:.5f}\")\n",
    "\n",
    "        # update \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        lear_rate.append(scheduler.get_last_lr())\n",
    "        \n",
    "        \n",
    "    test_acc = 0\n",
    "    \n",
    "    #values stored for calculatinf the confusion matrix\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    #starting the test\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.float()\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            test_acc += (pred == labels).sum().item()\n",
    "            \n",
    "            lab_pred.extend(pred.cpu().numpy())\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            lab_true.extend(labels)\n",
    "    print(f'Test Accuracy:  {(test_acc / len(test_loader))}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return history, lab_true, lab_pred, lear_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:10:01.567652Z",
     "iopub.status.busy": "2024-03-14T12:10:01.567230Z",
     "iopub.status.idle": "2024-03-14T12:23:14.922284Z",
     "shell.execute_reply": "2024-03-14T12:23:14.921222Z",
     "shell.execute_reply.started": "2024-03-14T12:10:01.567621Z"
    },
    "id": "1BsMfn4S-Ctn",
    "outputId": "78591bfe-d12b-4d62-d7d3-5b924908835f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "hist, lab_true, lab_pred, lr = train_model(chosen_model, train_dataset, val_dataset, test_dataset, device, lr=0.01, batch_size=32, epochs=35, l2=0.0001\n",
    "                   , patience=5, gamma = 0.5, freezed_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:25:28.191037Z",
     "iopub.status.busy": "2024-03-14T12:25:28.190591Z",
     "iopub.status.idle": "2024-03-14T12:25:29.843340Z",
     "shell.execute_reply": "2024-03-14T12:25:29.842230Z",
     "shell.execute_reply.started": "2024-03-14T12:25:28.191005Z"
    },
    "id": "gy9D6zHY-Cto",
    "outputId": "44788efb-c5f4-47f2-edf3-7c46bdf60d0e"
   },
   "outputs": [],
   "source": [
    "#changing to appropriate format for matplotlib\n",
    "train_loss = torch.tensor(hist['train_loss'])\n",
    "val_loss = torch.tensor(hist['val_loss'])\n",
    "train_acc = torch.tensor(hist['train_acc'])\n",
    "val_acc =  torch.tensor(hist['val_acc'])\n",
    "\n",
    "\n",
    "# plot training curves\n",
    "epochs = range(1, len(hist['train_loss']) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,7))\n",
    "ax[0].plot(epochs, train_loss, 'r-', label='Train')\n",
    "ax[0].plot(epochs, val_loss , 'b-', label='Evaluation')\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epochs, train_acc, 'r-', label='Train')\n",
    "ax[1].plot(epochs, val_acc , 'b-', label='Evaluation')           \n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Acc')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T12:25:40.183160Z",
     "iopub.status.busy": "2024-03-14T12:25:40.182813Z",
     "iopub.status.idle": "2024-03-14T12:25:40.762503Z",
     "shell.execute_reply": "2024-03-14T12:25:40.761434Z",
     "shell.execute_reply.started": "2024-03-14T12:25:40.183135Z"
    }
   },
   "outputs": [],
   "source": [
    "#create confusion matrix\n",
    "cf_matrix = confusion_matrix(lab_true, lab_pred)\n",
    "classes = GalaxyMNIST.classes\n",
    "df_cm = pd.DataFrame(cf_matrix \n",
    "                     , index = [i for i in classes], columns = [i for i in classes])\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize = (10,6))\n",
    "sn.heatmap(df_cm, annot=True, fmt = \".0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
