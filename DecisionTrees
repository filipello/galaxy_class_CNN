################################        DECISION TREES        #############################
#    INDEX
#  1. GINI INDEX
#  2. ENTROPY
#  3. BAGGING
#  4. XGBOOST


# ho decisamente importato più di quanto necessario, avevo copiato dal codice della rete neurale

from PIL import Image
import numpy as np
import pandas as pd
import glob
import torch
import matplotlib.pyplot as plt # data visualization
import seaborn as sns # statistical data visualization
%matplotlib inline
import os
!pip install Pillow
import seaborn as sn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import xgboost as xgb

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

%pip install git+https://github.com/mwalmsley/galaxy_mnist.git
from galaxy_mnist import GalaxyMNIST
#from galaxy_mnist import GalaxyMNISTHighrez

#  Import galaxyMNIST dataset
train_dataset = GalaxyMNIST(
    root='ML_DP/gal_mnist',
    download=True
)

test_dataset = GalaxyMNIST(
    root='ML_DP/gal_mnist',
    download=True,
    train=False
)

images_train=train_dataset.data
y_train = train_dataset.targets
images_val=test_dataset.data
y_val = test_dataset.targets
#galaxies=np.array(data_images)

print(images_train[0].shape) # it is 3-dimensional, So we have to re-shape it #QUESTA LINEA SI PUO' TOGLIERE, SERVE SOLO A MOTIVARE IL RESHAPE
nsamples , n , nx, ny = images_train.shape
d2_images_train = images_train.reshape((nsamples,n*nx*ny))
nsamples , n , nx, ny = images_val.shape
d2_images_val = images_val.reshape((nsamples,n*nx*ny))

############################    GINI INDEX    ########################################################

from sklearn import tree

classifier_gini_nomax = tree.DecisionTreeClassifier(criterion='gini', random_state=0)
classifier_gini_nomax.fit(d2_images_train, y_train)

# vediamo il numero di nodi e la max depth attuale dell'albero
n_nodes = classifier_gini_nomax.tree_.node_count
print(n_nodes)
print(classifier_gini_nomax.tree_.max_depth)

# vogliamo capire qual è il max depth ideale. facciamo un 5-fold cross-validation
values = np.arange(3,27,3) # il 27 viene dal passo prima. 
from sklearn.model_selection import cross_val_score
cv_scs = {}
for i in values:
    clf = tree.DecisionTreeClassifier(max_depth=i,random_state=0)
    cv_scs[i]= cross_val_score(clf, d2_images_train, y_train, cv=5, n_jobs=-1)

error_score=[]
for i in cv_scs.values():
    error_score.append(1-i)
plt.plot(cv_scs.keys(),error_score)
plt.show()

# focus attorno a 9
values_new = [6,7,8,9,10,11] # proviamo a vedere se tra 6 e 11 ci sono valori migliori
#from sklearn.model_selection import cross_val_score
cv_scs_new = {}
for i in values_new:
    clf = tree.DecisionTreeClassifier(max_depth=i,random_state=0)
    cv_scs_new[i]= cross_val_score(clf, d2_images_train, y_train, cv=5, n_jobs=-1)

error_score_new=[]
for i in cv_scs_new.values():
    error_score_new.append(1-i)
plt.plot(cv_scs_new.keys(),error_score_new)
plt.show()

classifier_gini = tree.DecisionTreeClassifier(criterion='gini', max_depth=7, random_state=0) # la max_depth viene dal passo precedente, ed è 7 / 9
classifier_gini.fit(d2_images_train, y_train) # fittiamo con le immagini del train

#vediamo il numero di nodi e la max depth attuale dell'albero
n_nodes = classifier_gini.tree_.node_count
print(n_nodes)
print(classifier_gini.tree_.max_depth)

# predict in test set
y_pred_gini = classifier_gini.predict(d2_images_val) # ora usiamo l albero per predire le labels delle immagini in test 
print('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_val, y_pred_gini)))

# check sul train set

y_pred_train_gini = classifier_gini.predict(d2_images_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))

# proviamo a disegnare l'albero, anche se è troppo grande per poterlo vedere bene
plt.figure(figsize=(15,15))
tree.plot_tree(classifier_gini.fit(d2_images_train, y_train))

# confusion matrix
cf_matrix = confusion_matrix(y_val, y_pred_gini)
classes = GalaxyMNIST.classes
df_cm = pd.DataFrame(cf_matrix 
                     , index = [i for i in classes], columns = [i for i in classes])
plt.figure(figsize = (10,6))
sn.heatmap(df_cm, annot=True)

############################    ENTROPY    ########################################################

classifier_entropy_nomax = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier_entropy_nomax.fit(d2_images_train, y_train)
#vediamo il numero di nodi e la max depth attuale dell'albero
n_nodes = classifier_entropy_nomax.tree_.node_count
print(n_nodes)
print(classifier_entropy_nomax.tree_.max_depth)

# vogliamo capire qual è il max depth ideal. facciamo un 5-fold cross-validation
values = np.arange(2,20,3) # il 20 viene dal passo prima. 
from sklearn.model_selection import cross_val_score
cv_scs = {}
for i in values:
    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=i,random_state=0)
    cv_scs[i]= cross_val_score(clf, d2_images_train, y_train, cv=5, n_jobs=-1)
error_score=[]
for i in cv_scs.values():
    error_score.append(1-i)
plt.plot(cv_scs.keys(),error_score)
plt.show()

# vediamo se ci sono valori migliori attorno a 8
values_new = [6,7,8,9,10]
from sklearn.model_selection import cross_val_score
cv_scs_new = {}
for i in values_new:
    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=i,random_state=0)
    cv_scs_new[i]= cross_val_score(clf, d2_images_train, y_train, cv=5, n_jobs=-1)
error_score_new=[]
for i in cv_scs_new.values():
    error_score_new.append(1-i)
plt.plot(cv_scs_new.keys(),error_score_new)
plt.show()

classifier_entropy = tree.DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=0) #max_depth vista dal passo precedente
# fit the model
classifier_entropy.fit(d2_images_train, y_train)

#vediamo il numero di nodi e la max depth attuale dell'albero
n_nodes = classifier_entropy.tree_.node_count
print(n_nodes)
print(classifier_entropy.tree_.max_depth)

y_pred_entropy = classifier_entropy.predict(d2_images_val)
print('Model accuracy score with criterion entropy index: {0:0.4f}'. format(accuracy_score(y_val, y_pred_entropy)))

y_pred_train_entropy = classifier_entropy.predict(d2_images_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_entropy)))

#confusion matrix

cf_matrix = confusion_matrix(y_val, y_pred_entropy)
classes = GalaxyMNIST.classes
df_cm = pd.DataFrame(cf_matrix 
                     , index = [i for i in classes], columns = [i for i in classes])
plt.figure(figsize = (10,6))
sn.heatmap(df_cm, annot=True)


############################    BAGGING    ########################################################

from sklearn import ensemble

bagging_model = ensemble.BaggingClassifier(tree.DecisionTreeClassifier(max_depth = 9), n_estimators = 15, max_samples = 0.50, max_features = 0.50, random_state=42) # bootstrap, di default è true
bagging_model.fit(d2_images_train, y_train)

y_train_predBag = bagging_model.predict(d2_images_train)

print(confusion_matrix(y_train, y_train_predBag))
print(accuracy_score(y_train, y_train_predBag))

y_val_predBag = bagging_model.predict(d2_images_val)

print(accuracy_score(y_val, y_val_predBag))

#confusion matrix
cf_matrix = confusion_matrix(y_val, y_val_predBag)
classes = GalaxyMNIST.classes
df_cm = pd.DataFrame(cf_matrix 
                     , index = [i for i in classes], columns = [i for i in classes])
plt.figure(figsize = (10,6))
sn.heatmap(df_cm, annot=True)



############################    XGBOOST    ########################################################

xgb_model = xgb.XGBClassifier(objective='multi:softmax', tree_method='hist',  device='cuda', max_depth=2, random_state=42)
xgb_model.fit(d2_images_train, y_train)

y_train_pred = xgb_model.predict(d2_images_train)

print(confusion_matrix(y_train, y_train_pred))
print(accuracy_score(y_train, y_train_pred))

y_val_pred = xgb_model.predict(d2_images_val)
print(accuracy_score(y_val, y_val_pred))

#confusion matrix
cf_matrix = confusion_matrix(y_val, y_val_pred)
classes = GalaxyMNIST.classes
df_cm = pd.DataFrame(cf_matrix 
                     , index = [i for i in classes], columns = [i for i in classes])
plt.figure(figsize = (10,6))
sn.heatmap(df_cm, annot=True)
